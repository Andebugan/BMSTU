\title{Лекции по математической статистике}
\chapter{Лекция 5}
\section{Интервальный статистический ряд}
Выше было понятие статистического ряда. Однако, если объем достаточно велик (n > 50), то элементы выборки группируют в так называемый интервальный статистический ряд. Для этого отрезок $J = [x_{(1)}, x_{(n)}]$ разбивают на m равновеликих промежутков. Ширина каждого из них $\Delta = \frac{|J|}{m} = \frac{x_{(1)} - x_{(n)}}{n}$. Данные промежутки строятся по следующему правилу:\\
$J_{i} = [x_{(1)} + (i - 1)\Delta; x_{(i)} + i\Delta), i=\overline{1, m-1}$\\
$J_{m} = [x_{(1)} + (m - 1)\Delta; x_{(n)}]$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_1}}
\end{figure}

Определение интервального статистического ряда, отвечающего выборке $x$ называется таблица следующего вида:\\

\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_2}}
\end{figure}

$n_{i}$ - число элементов выблоки $\overrightarrow{x}$, попавших в промежуток $J_{i}, i = \overline{1, m}$\\
Замечание:\\
1) $\sum\limits_{i = 1}^{m} n_{i} = n$\\
2) Для выбора m используют формулу:\\
$m = [log_{2}n] + 2$\\
или\\
$m = [log_{2}n] + 1$\\

\section{Эмпирическая плотность}
Пусть для данной выборки $\overrightarrow{x}$ построен интервальный статистический ряд $(J_{i}, n_{i}), i = \overline{1, m}$\\
Определение:\\
Эмпирической плотностью распределения соответствующей выборки $\overrightarrow{x}$ называется функция:\\
\begin{equation}
f_{n}(x) = 
\begin{cases}
\frac{n_{i}}{n \cdot \Delta}, x \in J_{i}\\
0
\end{cases}
\end{equation}

Замечание:
1) Очевидно, что $\int\limits_{-\infty}^{+\infty} f_{n} (x) dx = \int\limits_{x_{(1)}}^{x_{(m)}} f_{n} (x) dx = \sum\limits_{i=1}^{m}\frac{n_{i}}{n \cdot \Delta} \Delta = 1$\\
Таким образом эмпирическая плотность распределения удовлетворяет условию нормировки. Легко показать, что она обладает всеми свойствами функции плотности распределения.\\
2) $f_{n}(x)$ является кусочно-постоянной функцией:
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_3}}
\end{figure}

3) Функция $f_{n}(x)$ вяляется статистическим аналогом функции плотности распределения вероятности. Доказательство - аналогично доказанному выше результату для функции распределения. $\hat{F}_{n}(x) \overrightarrow{x \rightarrow \infty} F(x)$ на P\\

$f_{n}(x)$ примерно равна $f(x)$ при n >> 1.

Опредениение - график эмпирической функции плотности называется гистограммой.

\section{Полигон частот}
Определение полигона частот - пусть для некоторой выборки $\overrightarrow{x}$ построены гистограммы, по определению полигоном частот называется ломаная, звенья которой соединяют середины верних сторон соседних прямоугольников гистограммы.
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_4}}
\end{figure}

\section{Некоторые распределения, используемые в математической статистике}
\subsection{Гамма-функция Эйлера}
По определению гамма-функцией Эйлера называется выражение $\Gamma: R^{+} \rightarrow R$, определённое правилом:\\
$\Gamma(x) = \int\limits_{0}^{+\infty}e^{-t} t^{x-1}dt$\\

Замечание:\\
1) Интерграл является несобственным первого рода при $x \geqslant 1$;\\
при $x \in (0; 1)$ этот интеграл является несобственным и имеет следующие особенности: в t = 0 - подинтегральная функция имеет разрыв второго рода, верхний предел равен бесконечности. Легко проверить, что данный интеграл сходится при $x > 0$, при остальных вещественных x он расходится.\\

Некоторые свойства гамма-функции:\\
1. $\Gamma(x)$ - является бесконечное число раз дифференцируемой функцией, при этом её к-ая производная задаётся следующей формулой:\\
$\Gamma^{k}(x) = \int\limits_{0}^{+\infty} e^{-t} t^{x - 1} (ln t)^{k} dt$\\
2. $\Gamma(x + 1) = x\Gamma(x), x > 0$\\
3. $\Gamma(1) = 1$\\
4. $\Gamma(n + 1) = n!, n \in N$, по этой причине часто говорят, что гамма-функция является обобщением понятия факториала на вещественные числа.\\
5. $\Gamma(\frac{1}{2}) = \sqrt{\pi}$, вывод через интеграл Пуассона.
6. $\Gamma(\frac{n+1}{2}) = \bigg| $по второму свойству$\bigg| = \frac{n-1}{2}\Gamma(\frac{n-1}{2}) = ... = \frac{n-1}{2} \frac{n-2}{2} ...  \frac{1}{2} \Gamma(\frac{n-1}{2}) = \frac{1 \cdot 3 \cdot 5 ... \cdot (n - 1)}{2^{n}}\sqrt{\pi}$\\
7. Эскиз графика $\Gamma(x)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_5}}
\end{figure}

\subsection{Гамма-распределение}
Определение: говорят, что случайная величина $\xi$ имеет гамма-распределение, ели её функция плотности распределения вероятности имеет вид:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x}, x  > 0\\

\end{cases}
\end{equation}
Обозначаеся как $\xi ~ \Gamma(\lambda, \alpha)$

Замечание:\\
1) Экспоненциальное распределениe:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\lambda e^{-\lambda x}, x  > 0\\
0
\end{cases}
\end{equation}
$Exp(\lambda) = \Gamma(\lambda, 1)$\\

Теорема:\\
Пусть случайная величина $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, а $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, $\xi_{1}$ и $\xi_{2}$ - независимы. Тогда:\\
$\xi_{1} + \xi_{2} ~ \Gamma(\lambda, \alpha_{1} + \alpha_{2})$\\

Следствие:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, причём $\xi_{i} ~ \Gamma(\lambda, \alpha_{i}), i = \overline{1, n}$, то:\\
$\xi_{1} + ... + \xi_{n} ~ \Gamma(\lambda, \alpha_{1} + ... + \alpha_{n})$\\

\subsection{Распределение Релея}
Пусть $\xi ~ \mathcal{N}(0, \sigma^{2})$\\
Говорят, что случайная величина $\xi$ имеет распределения Релея с параметром $\sigma$.

Замечание:\\
1) Несложно показать, что:\\
\begin{equation}
f_{y}(x) = 
\begin{cases}
\frac{1}{\sigma\sqrt{2\pi x}}e^{\frac{-x}{2b^{2}}}, x > 0\\
0
\end{cases}
\end{equation}

2) Распределение Релея является частным случаем гамма-распределения для $\lambda = \frac{1}{2\sigma^{2}}$ и $\lambda = \frac{1}{2}$, то есть $\nu ~ \Gamma(\frac{1}{2\sigma^{2}}, \frac{1}{2})$\\

\subsection{Распределение хи-квадрат}
Пусть:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, $\xi_{i} ~ N(0, 1), i = \overline{1, n}$, $\nu = \xi_{1}^{2} + ... + \xi_{n}^{2}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение хи-квадрат с n степенями свободы. Обозначается как $\nu ~ X^{2}(n)$\\

Замечание:\\
1)  $\xi_{i} ~ N(0, 1) \Rightarrow \xi^{2}_{i}$ имеет распределение Релея с параметром $\sigma = 1$, то есть $\xi^{2}_{i} ~ \Gamma(\frac{1}{2}, \frac{1}{2})$. Так как случайные величины $\xi_{1} ... \xi_{n}$ - независимы с учётом свойства гамма-распределения:\\
$\nu = \xi_{1}^{2} + ... + \xi^{2}_{n} ~ \Gamma(\frac{1}{2}, \frac{n}{2})$, то $X^{2} = \Gamma(\frac{1}{2}, \frac{n}{2})$\\
2) Очевидно, что если независимые случайные величины $\nu_{1}, ... \nu_{m}$ имеют распределения $X^{2} (\nu_{i} ~ X^{2} (k_{i}), i=\overline{1, m})$, то $\nu_{1} + ... + \nu_{n} ~ X^{2}(k_{1} + ... k_{m})$\\
3)График функции плотности $\nu ~ X^{2}(n)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_6}}
\end{figure}

\subsection{Распределение Фишера}
Пусть:\\
1) $\xi_{1}, \xi_{2}$ - независимы
2) $\xi_{i} ~ X^{2}(n_{i}), i = \overline{1,2}$\\
3) $\nu = \frac{n_{1} \xi_{1}}{n_{2} \xi_{2}}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение Фишера со степенями свободы $n_{1} и n_{2}$, $\nu ~ F(n_{1}, n_{2})$\\

Замечания:\\
1) Можно показать, что:\\
\begin{equation}
f_{\nu}(x) = 
\begin{cases}
C \frac{x^{\frac{n_{1}}{2} - 1}}{(1 + \frac{n_{1} x}{n_{2}})^{\frac{n_{1} + n_{2}}{2}}}, x > 0\\
0
\end{cases}
\end{equation}

$C = \frac{(\frac{n_{1}}{n_{2}})^{\frac{n_{1}}{2}}}{B(\frac{n_{1}}{2}, \frac{n_{2}}{2})}$\\
$B(x, y) = \int\limits_{0}^{1} t^{x - 1} (1 - t)^{y - 1} dt$  - бета-функция Эйлера.\\
2) Если $\nu ~ F(n_{1}, n_{2})$, то $\frac{1}{\nu} ~ F(n_{2}, n_{1})$\\

\chapter{Лекция 6}
\section{Переписать}

\chapter{Лекция 7}
По определению оценка $\hat{\theta}$ называется эффективной оценкой параметра $\theta$, если:\\
1) $\hat{\theta}$ является наименьшей оценкой для теты\\
2) оценка $\hat{\theta}$ обладает наименьшей дисперсией среди всех несмещённых $\theta$

Замечание: иногда говорят не об эффективной вообще точечной оценке, а об оценке, эффективной в некотором классе. Пусть $\Theta$ - некоторый класс несмещённых оценок для параметра $\theta$.\\
По определению оценка $\hat{\theta}$ называется эффективной в классе $\Theta$, если она имеет наименьшую дисперсию среди всех оценок этого класса, т.е. - $(\forall \tilde{\theta})(D\hat{\theta} \leqslant D\tilde{\theta})$.

Пример:\\
Пусть X - случайная величина, обладающая MX = m и DX = $b^2$. Покажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ является эффективной оценкой для m и b в классе линейных оценок.\\

Решение:\\
1) Линейная оценка имеет вид: $\hat{m}(\overline{X}) = \sum\limits_{i = 1}^{n} \lambda_{i} X_{i} = \lambda_{1} X_{1} + ... + \lambda_{n} X_{n} (*)$\\
где $\lambda_{i} \in R, i  = \overline{1,n}$, тогда матожидание линейной оценки (*) :
a) $M[\hat{m}] = \lambda_{1} M X_{1} + ... + \lambda_{n} M X_{n} = \bigg| X_{i} ~ X_{j}, MX = m\bigg| = (\lambda_{1} + ... + \lambda_{n})m$. Так как оценка является несмещённой, то $M[\hat{m}] = m \Rightarrow \sum\limits_{i=1}^{n} = 1$\\
b) Дисперсия оценки (*):\\
$D[\hat{m}] = \sum\limits_{i=1}^{n} \lambda_{i}^{2} DX_{i} = \lambda^{2} \sum_{i=1}^{n} \lambda_{i}^{2}$ - аналогично матожиданию.\\

2) Попробуем подобрать коэффициент $\lambda_{i}, i = \overline{1, n}$, и (*) так, чтобы:\\
\begin{equation}
\begin{cases}
D[\hat{m}] \rightarrow min\\
\sum\limits_{i=1}^{n} \lambda_{i} = 1\\
\end{cases}
\end{equation}

Для этого нужно решить задачу условной оптимизации:\\
\begin{equation}
\begin{cases}
\phi(\lambda_{1} ... \lambda_{n}) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} \rightarrow min\\
\sum\lambda_{i} = 1
\end{cases}
\end{equation}

Запишем функцию Лагранжа:\\
$L(\lambda_{1} ... \lambda_{n}, \mu) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} - \mu\sum\lambda_{i} - \mu$\\

\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 0\\
\frac{dL}{d\mu} = 0\\
\end{cases}
\end{equation}

Следовательно:\\
\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 2\lambda_{i} - \mu = 0\\
\frac{dL}{d\mu} = - (\sum\limits_{i=1}^{n} \lambda_{i} - 1) = 0\\
\end{cases}
\end{equation}

Из n первых уравнений - $\lambda_{i} = \frac{\mu}{2}, i = \overline{1, n}$\\

Покажем, что найденное решение соответствует точке условного минимума целевой функции, таким образом, подставляя $\lambda_{i}$ в * получаем искомую оценку с минимальной дисперсией в классе линейных оценок.\\
$\hat{m}(\overrightarrow{X}) = \frac{1}{n}X_{1} + ... + \frac{1}{n} X_{n} = \overline{X}$\\
Дисперсия этой оценке:\\
$D[\hat{m}] = \sigma^{2}\sum_{i=1}^{n}\lambda_{i}^{2} = \frac{\sigma^{2}}{n}$\\

Теорема:\\
Теорема о единственности эффективной оценки:\\
Пусть $\tilde{\theta_{1}}(\overline{X})$ и $\tilde{\theta_{2}}(\overline{X})$ - эффективные оценки некой оценки параметра $\theta$, тогда $\tilde{\theta_{1}}(\overline{X}) = \tilde{\theta_{2}}(\overline{X})$

\section{Неравенство Рао-Крамера}
Пусть X - случайная величина, закон распределения которой зависит от вектора $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{n})$ параметров.\\
Пусть $\overrightarrow{X} = (X_{1}, ..., X_{n})$ - случайная выборка из генеральной совокупности X.\\

Опеределение - функцией правдоподобия, отвечающей случайной выборке  $\overrightarrow{X}$ называется функция $L(\overrightarrow{X}, \overrightarrow{\theta}) = p(X_{1}, \overrightarrow{\theta}) ... p(X_{1}, \overrightarrow{\theta})$\\
где:\\
\begin{equation}
p(X_{i}, \overrightarrow{\theta}) = 
\begin{cases}
f(X_{i}, \overrightarrow{\theta}), \text{если X - непрерывная случайная величина}\\
P(X = X_{i}), \text{если X - дискретная случайная величина}\\
\end{cases}
\end{equation}

Пусть r = 1, т.е. $\overrightarrow{\theta} = (\theta_{1}) = (\theta)$\\

Определение - количество информации по Фишеру, содержащееся в случайной величине $\overrightarrow{X}$, называется число $I(\theta) = M[(\frac{d ln L}{d\theta})^{2}]$\\

Теорема:\\
Неравенство Рао-Крамера:\\
Пусть рассматриваемая модель является регулярной, $\hat{\theta}(\overrightarrow{X})$ - несмещённая оценка параметра тета. Тогда:\\
$D[\hat{\theta}] \geqslant \frac{1}{I(\theta)}$ - неравенство Рао-Крамера.\\

Замечание:\\
1) При доказательстве теоремы Рао используются дифференциальные параметры под знаком интеграла:\\
$\frac{d}{d\phi}\int\limits_{G}\phi(\overrightarrow{X}, \theta)dx = \int\limits_{G} \frac{d\phi(\overrightarrow{X}, \theta)}{d\theta}dx$\\
Т.е. параметрические модели, для которых справедливо это равенство, будем называть регулярными. \\
2) Неравенство Рао даёт нижнюю границу для дисперсии для всех возможных оценок параметра $\theta$.\\
3) Величина $e(\hat{\theta}) = \frac{1}{I(\theta)D(\hat{\theta})}$ называется показателем эффективности по Рао точечной оценки $\hat{\theta}$\\
$0 \leqslant e(\hat{\theta}) \leqslant 1$\\
Очевидно, что оценка эффективная по Рао будет "просто"  эффективной. Вопрос в том, для каких параметричесих моделей существует эффективная по Рао оценка (то есть существует оценка, дисперсия которой равна $\frac{1}{I(\theta)}$) мы оставим без рассмотрения.\\
4) В некоторых случаях вводят в рассмотрение величину $I_{0} (\theta) = M[(\frac{dp(X, \theta)}{d\theta})^{2}]$\\
где $p(X, \theta)$ имеет тот же смысл, что и функция правдоподобия.\\
Данную величину можно назвать количеством информации по Фишеру в одном испытании. Для некоторых параметрических моделей справедливо:\\
$I(\theta) = nI_{0}(\theta)$,\\
где n - объём случайной информации.\\

Пример:\\
Пусть $X ~ N(m, \sigma^{2})$, где m - неизвестна, $\sigma$ - известна. Докажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ для m является эффективной по Рао.\\
1) Необходимо найти показатель эффективности оценки $\hat{m_{1}}$:\\
$e(\hat{m}) = \frac{1}{I(m)D(\hat{m})}$, если данная величина равна 1, то оценка эффективна по Рао, иначе не является эффективной по Рао.
2) $D[\hat{m}] = D[\overline{X}] = ... = \frac{\sigma^{2}}{n}$\\
3) $I(\hat{m}) = ?$\\
$I(\hat{m})=M[(\frac{d ln L}{dn})^{2}]$, составим функцию L правдоподобия:\\
$L(\overline{X}, m) = p(X, m)\cdot ... \cdot p(X_{n}, m) = \frac{1}{(\sqrt{2 \pi})^{n} \sigma^{n}}e^{-\frac{1}{2\sigma^{2}}\sum(x_{i}-m)^{2}}$\\
Тогда:\\
$ln L(\overline{X}, n) = -\frac{n}{2}ln 2\pi - n ln \sigma - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(m - X_{i})^{2}$\\
$\frac{dln L(\overline{X}, m)}{dm} = -\frac{2}{2\sigma^{2}}\sum\limits_{i=1}^{n}(m - X_{i}) = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i} - m)$\\
$(\frac{dln L(\overline{X}, m)}{dm})^{2} = \frac{1}{\sigma^{4}}[(X_{1} - m) + ... + (X_{n} - m)]^{2}$\\
Т.о.:\\
$I(m) = M[(\frac{d ln L(\overline{X}, m)}{dm})^{2}] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n}M[(X_{i} - m)^{2} + 2\sum\limits_{1 \leqslant i < j \leqslant n}M[(X_{i} - m)(X_{j} - m)]] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n} \sigma^{2} + 0] = \frac{1}{\sigma^{4}} n\sigma^{2} = \frac{n}{\sigma^{2}}$\\
4) Получаем $e(\hat{m})$\\