\title{Лекции по математической статистике}
\chapter{Лекция 5}
\section{Интервальный статистический ряд}
Выше было понятие статистического ряда. Однако, если объем достаточно велик (n > 50), то элементы выборки группируют в так называемый интервальный статистический ряд. Для этого отрезок $J = [x_{(1)}, x_{(n)}]$ разбивают на m равновеликих промежутков. Ширина каждого из них $\Delta = \frac{|J|}{m} = \frac{x_{(1)} - x_{(n)}}{n}$. Данные промежутки строятся по следующему правилу:\\
$J_{i} = [x_{(1)} + (i - 1)\Delta; x_{(i)} + i\Delta), i=\overline{1, m-1}$\\
$J_{m} = [x_{(1)} + (m - 1)\Delta; x_{(n)}]$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_1}}
\end{figure}

Определение интервального статистического ряда, отвечающего выборке $x$ называется таблица следующего вида:\\

\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_2}}
\end{figure}

$n_{i}$ - число элементов выблоки $\overrightarrow{x}$, попавших в промежуток $J_{i}, i = \overline{1, m}$\\
Замечание:\\
1) $\sum\limits_{i = 1}^{m} n_{i} = n$\\
2) Для выбора m используют формулу:\\
$m = [log_{2}n] + 2$\\
или\\
$m = [log_{2}n] + 1$\\

\section{Эмпирическая плотность}
Пусть для данной выборки $\overrightarrow{x}$ построен интервальный статистический ряд $(J_{i}, n_{i}), i = \overline{1, m}$\\
Определение:\\
Эмпирической плотностью распределения соответствующей выборки $\overrightarrow{x}$ называется функция:\\
\begin{equation}
f_{n}(x) = 
\begin{cases}
\frac{n_{i}}{n \cdot \Delta}, x \in J_{i}\\
0
\end{cases}
\end{equation}

Замечание:
1) Очевидно, что $\int\limits_{-\infty}^{+\infty} f_{n} (x) dx = \int\limits_{x_{(1)}}^{x_{(m)}} f_{n} (x) dx = \sum\limits_{i=1}^{m}\frac{n_{i}}{n \cdot \Delta} \Delta = 1$\\
Таким образом эмпирическая плотность распределения удовлетворяет условию нормировки. Легко показать, что она обладает всеми свойствами функции плотности распределения.\\
2) $f_{n}(x)$ является кусочно-постоянной функцией:
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_3}}
\end{figure}

3) Функция $f_{n}(x)$ вяляется статистическим аналогом функции плотности распределения вероятности. Доказательство - аналогично доказанному выше результату для функции распределения. $\hat{F}_{n}(x) \overrightarrow{x \rightarrow \infty} F(x)$ на P\\

$f_{n}(x)$ примерно равна $f(x)$ при n >> 1.

Опредениение - график эмпирической функции плотности называется гистограммой.

\section{Полигон частот}
Определение полигона частот - пусть для некоторой выборки $\overrightarrow{x}$ построены гистограммы, по определению полигоном частот называется ломаная, звенья которой соединяют середины верних сторон соседних прямоугольников гистограммы.
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_4}}
\end{figure}

\section{Некоторые распределения, используемые в математической статистике}
\subsection{Гамма-функция Эйлера}
По определению гамма-функцией Эйлера называется выражение $\Gamma: R^{+} \rightarrow R$, определённое правилом:\\
$\Gamma(x) = \int\limits_{0}^{+\infty}e^{-t} t^{x-1}dt$\\

Замечание:\\
1) Интерграл является несобственным первого рода при $x \geqslant 1$;\\
при $x \in (0; 1)$ этот интеграл является несобственным и имеет следующие особенности: в t = 0 - подинтегральная функция имеет разрыв второго рода, верхний предел равен бесконечности. Легко проверить, что данный интеграл сходится при $x > 0$, при остальных вещественных x он расходится.\\

Некоторые свойства гамма-функции:\\
1. $\Gamma(x)$ - является бесконечное число раз дифференцируемой функцией, при этом её к-ая производная задаётся следующей формулой:\\
$\Gamma^{k}(x) = \int\limits_{0}^{+\infty} e^{-t} t^{x - 1} (ln t)^{k} dt$\\
2. $\Gamma(x + 1) = x\Gamma(x), x > 0$\\
3. $\Gamma(1) = 1$\\
4. $\Gamma(n + 1) = n!, n \in N$, по этой причине часто говорят, что гамма-функция является обобщением понятия факториала на вещественные числа.\\
5. $\Gamma(\frac{1}{2}) = \sqrt{\pi}$, вывод через интеграл Пуассона.
6. $\Gamma(\frac{n+1}{2}) = \bigg| $по второму свойству$\bigg| = \frac{n-1}{2}\Gamma(\frac{n-1}{2}) = ... = \frac{n-1}{2} \frac{n-2}{2} ...  \frac{1}{2} \Gamma(\frac{n-1}{2}) = \frac{1 \cdot 3 \cdot 5 ... \cdot (n - 1)}{2^{n}}\sqrt{\pi}$\\
7. Эскиз графика $\Gamma(x)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_5}}
\end{figure}

\subsection{Гамма-распределение}
Определение: говорят, что случайная величина $\xi$ имеет гамма-распределение, ели её функция плотности распределения вероятности имеет вид:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x}, x  > 0\\

\end{cases}
\end{equation}
Обозначаеся как $\xi ~ \Gamma(\lambda, \alpha)$

Замечание:\\
1) Экспоненциальное распределениe:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\lambda e^{-\lambda x}, x  > 0\\
0
\end{cases}
\end{equation}
$Exp(\lambda) = \Gamma(\lambda, 1)$\\

Теорема:\\
Пусть случайная величина $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, а $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, $\xi_{1}$ и $\xi_{2}$ - независимы. Тогда:\\
$\xi_{1} + \xi_{2} ~ \Gamma(\lambda, \alpha_{1} + \alpha_{2})$\\

Следствие:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, причём $\xi_{i} ~ \Gamma(\lambda, \alpha_{i}), i = \overline{1, n}$, то:\\
$\xi_{1} + ... + \xi_{n} ~ \Gamma(\lambda, \alpha_{1} + ... + \alpha_{n})$\\

\subsection{Распределение Релея}
Пусть $\xi ~ \mathcal{N}(0, \sigma^{2})$\\
Говорят, что случайная величина $\xi$ имеет распределения Релея с параметром $\sigma$.

Замечание:\\
1) Несложно показать, что:\\
\begin{equation}
f_{y}(x) = 
\begin{cases}
\frac{1}{\sigma\sqrt{2\pi x}}e^{\frac{-x}{2b^{2}}}, x > 0\\
0
\end{cases}
\end{equation}

2) Распределение Релея является частным случаем гамма-распределения для $\lambda = \frac{1}{2\sigma^{2}}$ и $\lambda = \frac{1}{2}$, то есть $\nu ~ \Gamma(\frac{1}{2\sigma^{2}}, \frac{1}{2})$\\

\subsection{Распределение хи-квадрат}
Пусть:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, $\xi_{i} ~ N(0, 1), i = \overline{1, n}$, $\nu = \xi_{1}^{2} + ... + \xi_{n}^{2}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение хи-квадрат с n степенями свободы. Обозначается как $\nu ~ X^{2}(n)$\\

Замечание:\\
1)  $\xi_{i} ~ N(0, 1) \Rightarrow \xi^{2}_{i}$ имеет распределение Релея с параметром $\sigma = 1$, то есть $\xi^{2}_{i} ~ \Gamma(\frac{1}{2}, \frac{1}{2})$. Так как случайные величины $\xi_{1} ... \xi_{n}$ - независимы с учётом свойства гамма-распределения:\\
$\nu = \xi_{1}^{2} + ... + \xi^{2}_{n} ~ \Gamma(\frac{1}{2}, \frac{n}{2})$, то $X^{2} = \Gamma(\frac{1}{2}, \frac{n}{2})$\\
2) Очевидно, что если независимые случайные величины $\nu_{1}, ... \nu_{m}$ имеют распределения $X^{2} (\nu_{i} ~ X^{2} (k_{i}), i=\overline{1, m})$, то $\nu_{1} + ... + \nu_{n} ~ X^{2}(k_{1} + ... k_{m})$\\
3)График функции плотности $\nu ~ X^{2}(n)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_6}}
\end{figure}

\subsection{Распределение Фишера}
Пусть:\\
1) $\xi_{1}, \xi_{2}$ - независимы
2) $\xi_{i} ~ X^{2}(n_{i}), i = \overline{1,2}$\\
3) $\nu = \frac{n_{1} \xi_{1}}{n_{2} \xi_{2}}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение Фишера со степенями свободы $n_{1} и n_{2}$, $\nu ~ F(n_{1}, n_{2})$\\

Замечания:\\
1) Можно показать, что:\\
\begin{equation}
f_{\nu}(x) = 
\begin{cases}
C \frac{x^{\frac{n_{1}}{2} - 1}}{(1 + \frac{n_{1} x}{n_{2}})^{\frac{n_{1} + n_{2}}{2}}}, x > 0\\
0
\end{cases}
\end{equation}

$C = \frac{(\frac{n_{1}}{n_{2}})^{\frac{n_{1}}{2}}}{B(\frac{n_{1}}{2}, \frac{n_{2}}{2})}$\\
$B(x, y) = \int\limits_{0}^{1} t^{x - 1} (1 - t)^{y - 1} dt$  - бета-функция Эйлера.\\
2) Если $\nu ~ F(n_{1}, n_{2})$, то $\frac{1}{\nu} ~ F(n_{2}, n_{1})$\\

\chapter{Лекция 6}
\section{Переписать}

\chapter{Лекция 7}
По определению оценка $\hat{\theta}$ называется эффективной оценкой параметра $\theta$, если:\\
1) $\hat{\theta}$ является наименьшей оценкой для теты\\
2) оценка $\hat{\theta}$ обладает наименьшей дисперсией среди всех несмещённых $\theta$

Замечание: иногда говорят не об эффективной вообще точечной оценке, а об оценке, эффективной в некотором классе. Пусть $\Theta$ - некоторый класс несмещённых оценок для параметра $\theta$.\\
По определению оценка $\hat{\theta}$ называется эффективной в классе $\Theta$, если она имеет наименьшую дисперсию среди всех оценок этого класса, т.е. - $(\forall \tilde{\theta})(D\hat{\theta} \leqslant D\tilde{\theta})$.

Пример:\\
Пусть X - случайная величина, обладающая MX = m и DX = $b^2$. Покажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ является эффективной оценкой для m и b в классе линейных оценок.\\

Решение:\\
1) Линейная оценка имеет вид: $\hat{m}(\overline{X}) = \sum\limits_{i = 1}^{n} \lambda_{i} X_{i} = \lambda_{1} X_{1} + ... + \lambda_{n} X_{n} (*)$\\
где $\lambda_{i} \in R, i  = \overline{1,n}$, тогда матожидание линейной оценки (*) :
a) $M[\hat{m}] = \lambda_{1} M X_{1} + ... + \lambda_{n} M X_{n} = \bigg| X_{i} ~ X_{j}, MX = m\bigg| = (\lambda_{1} + ... + \lambda_{n})m$. Так как оценка является несмещённой, то $M[\hat{m}] = m \Rightarrow \sum\limits_{i=1}^{n} = 1$\\
b) Дисперсия оценки (*):\\
$D[\hat{m}] = \sum\limits_{i=1}^{n} \lambda_{i}^{2} DX_{i} = \lambda^{2} \sum_{i=1}^{n} \lambda_{i}^{2}$ - аналогично матожиданию.\\

2) Попробуем подобрать коэффициент $\lambda_{i}, i = \overline{1, n}$, и (*) так, чтобы:\\
\begin{equation}
\begin{cases}
D[\hat{m}] \rightarrow min\\
\sum\limits_{i=1}^{n} \lambda_{i} = 1\\
\end{cases}
\end{equation}

Для этого нужно решить задачу условной оптимизации:\\
\begin{equation}
\begin{cases}
\phi(\lambda_{1} ... \lambda_{n}) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} \rightarrow min\\
\sum\lambda_{i} = 1
\end{cases}
\end{equation}

Запишем функцию Лагранжа:\\
$L(\lambda_{1} ... \lambda_{n}, \mu) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} - \mu\sum\lambda_{i} - \mu$\\

\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 0\\
\frac{dL}{d\mu} = 0\\
\end{cases}
\end{equation}

Следовательно:\\
\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 2\lambda_{i} - \mu = 0\\
\frac{dL}{d\mu} = - (\sum\limits_{i=1}^{n} \lambda_{i} - 1) = 0\\
\end{cases}
\end{equation}

Из n первых уравнений - $\lambda_{i} = \frac{\mu}{2}, i = \overline{1, n}$\\

Покажем, что найденное решение соответствует точке условного минимума целевой функции, таким образом, подставляя $\lambda_{i}$ в * получаем искомую оценку с минимальной дисперсией в классе линейных оценок.\\
$\hat{m}(\overrightarrow{X}) = \frac{1}{n}X_{1} + ... + \frac{1}{n} X_{n} = \overline{X}$\\
Дисперсия этой оценке:\\
$D[\hat{m}] = \sigma^{2}\sum_{i=1}^{n}\lambda_{i}^{2} = \frac{\sigma^{2}}{n}$\\

Теорема:\\
Теорема о единственности эффективной оценки:\\
Пусть $\tilde{\theta_{1}}(\overline{X})$ и $\tilde{\theta_{2}}(\overline{X})$ - эффективные оценки некой оценки параметра $\theta$, тогда $\tilde{\theta_{1}}(\overline{X}) = \tilde{\theta_{2}}(\overline{X})$

\section{Неравенство Рао-Крамера}
Пусть X - случайная величина, закон распределения которой зависит от вектора $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{n})$ параметров.\\
Пусть $\overrightarrow{X} = (X_{1}, ..., X_{n})$ - случайная выборка из генеральной совокупности X.\\

Опеределение - функцией правдоподобия, отвечающей случайной выборке  $\overrightarrow{X}$ называется функция $L(\overrightarrow{X}, \overrightarrow{\theta}) = p(X_{1}, \overrightarrow{\theta}) ... p(X_{1}, \overrightarrow{\theta})$\\
где:\\
\begin{equation}
p(X_{i}, \overrightarrow{\theta}) = 
\begin{cases}
f(X_{i}, \overrightarrow{\theta}), \text{если X - непрерывная случайная величина}\\
P(X = X_{i}), \text{если X - дискретная случайная величина}\\
\end{cases}
\end{equation}

Пусть r = 1, т.е. $\overrightarrow{\theta} = (\theta_{1}) = (\theta)$\\

Определение - количество информации по Фишеру, содержащееся в случайной величине $\overrightarrow{X}$, называется число $I(\theta) = M[(\frac{d ln L}{d\theta})^{2}]$\\

Теорема:\\
Неравенство Рао-Крамера:\\
Пусть рассматриваемая модель является регулярной, $\hat{\theta}(\overrightarrow{X})$ - несмещённая оценка параметра тета. Тогда:\\
$D[\hat{\theta}] \geqslant \frac{1}{I(\theta)}$ - неравенство Рао-Крамера.\\

Замечание:\\
1) При доказательстве теоремы Рао используются дифференциальные параметры под знаком интеграла:\\
$\frac{d}{d\phi}\int\limits_{G}\phi(\overrightarrow{X}, \theta)dx = \int\limits_{G} \frac{d\phi(\overrightarrow{X}, \theta)}{d\theta}dx$\\
Т.е. параметрические модели, для которых справедливо это равенство, будем называть регулярными. \\
2) Неравенство Рао даёт нижнюю границу для дисперсии для всех возможных оценок параметра $\theta$.\\
3) Величина $e(\hat{\theta}) = \frac{1}{I(\theta)D(\hat{\theta})}$ называется показателем эффективности по Рао точечной оценки $\hat{\theta}$\\
$0 \leqslant e(\hat{\theta}) \leqslant 1$\\
Очевидно, что оценка эффективная по Рао будет ''просто'' эффективной. Вопрос в том, для каких параметричесих моделей существует эффективная по Рао оценка (то есть существует оценка, дисперсия которой равна $\frac{1}{I(\theta)}$) мы оставим без рассмотрения.\\
4) В некоторых случаях вводят в рассмотрение величину $I_{0} (\theta) = M[(\frac{dp(X, \theta)}{d\theta})^{2}]$\\
где $p(X, \theta)$ имеет тот же смысл, что и функция правдоподобия.\\
Данную величину можно назвать количеством информации по Фишеру в одном испытании. Для некоторых параметрических моделей справедливо:\\
$I(\theta) = nI_{0}(\theta)$,\\
где n - объём случайной информации.\\

Пример:\\
Пусть $X ~ N(m, \sigma^{2})$, где m - неизвестна, $\sigma$ - известна. Докажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ для m является эффективной по Рао.\\
1) Необходимо найти показатель эффективности оценки $\hat{m_{1}}$:\\
$e(\hat{m}) = \frac{1}{I(m)D(\hat{m})}$, если данная величина равна 1, то оценка эффективна по Рао, иначе не является эффективной по Рао.
2) $D[\hat{m}] = D[\overline{X}] = ... = \frac{\sigma^{2}}{n}$\\
3) $I(\hat{m}) = ?$\\
$I(\hat{m})=M[(\frac{d ln L}{dn})^{2}]$, составим функцию L правдоподобия:\\
$L(\overline{X}, m) = p(X, m)\cdot ... \cdot p(X_{n}, m) = \frac{1}{(\sqrt{2 \pi})^{n} \sigma^{n}}e^{-\frac{1}{2\sigma^{2}}\sum(x_{i}-m)^{2}}$\\
Тогда:\\
$ln L(\overline{X}, n) = -\frac{n}{2}ln 2\pi - n ln \sigma - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(m - X_{i})^{2}$\\
$\frac{dln L(\overline{X}, m)}{dm} = -\frac{2}{2\sigma^{2}}\sum\limits_{i=1}^{n}(m - X_{i}) = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i} - m)$\\
$(\frac{dln L(\overline{X}, m)}{dm})^{2} = \frac{1}{\sigma^{4}}[(X_{1} - m) + ... + (X_{n} - m)]^{2}$\\
Т.о.:\\
$I(m) = M[(\frac{d ln L(\overline{X}, m)}{dm})^{2}] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n}M[(X_{i} - m)^{2} + 2\sum\limits_{1 \leqslant i < j \leqslant n}M[(X_{i} - m)(X_{j} - m)]] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n} \sigma^{2} + 0] = \frac{1}{\sigma^{4}} n\sigma^{2} = \frac{n}{\sigma^{2}}$\\
4) Получаем $e(\hat{m})$\\

\chapter{Лекция 8}
\section{Методы построения точечных оценок}
1. Метод моментов\\
2. Метод максимального правдоподобия.\\

\section{Метод моментов}
Пусть:\\
1) X - некий случайный вектор, распередление которого зависит от вектора $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{n})$ неизвестных параметров.\\
2) $\exists r $ первых моментов случайного вектора X, то есть $\exists M[X^{k}], k = 1,.. r$\\

Тогда в методе моментов:\\
1) Вычисляются теоретические моменты 1-го, 2-го, ..., r-го порядков, зависящих от неизвестных параметров:\\
$m_{k}(\theta_{1}, ..., \theta_{r} = M[X^{k}], k = \overline{1, r}$ - теоретические моменты порядка k\\

2) Теоретические моменты приравниваются к выборочным аналогам:\\

\begin{equation}
\begin{cases}
m_{1}(\theta_{1}, ..., \theta_{r}) = \hat{m_{1}}(\overrightarrow{X})\\
...\\
m_{r}(\theta_{1}, ..., \theta_{r}) = \hat{m_{r}}(\overrightarrow{X})\\
\end{cases}
\end{equation}
Система уравнений (возможно, нелинейных), относительно неизвестных параметров тета.\\

3) Решаем получившуюся систему:\\
\begin{equation}
\begin{cases}
\theta_{1} = \hat{\theta_{1}}(\overrightarrow{X})\\
...\\
\theta_{r} = \hat{\theta_{r}}(\overrightarrow{X})\\
\end{cases}
\end{equation}

Полученные зависимости используются в качестве точечных оценок для полуинтервалов.\\

Замечание:\\
Иногда некоторые уравнения системы из подпункта (2) удобнее записывать относительно центральных, а не начальных моментов. В этом случает k-е уравнение будет иметь вид:\\
$\mathring{m_{k}}(\theta_{1}, ..., \theta_{r}) = \hat{\circ{m_{k}}}(\overrightarrow{X})$, где:\\
$\mathring{m_{k}}(\theta_{1}, ..., \theta_{r}) = M[(X - MX)^{k}]$\\
$\hat{\mathring{m_{k}}}(\overrightarrow{X}) = \frac{1}{n}\sum\limits_{i=1}^{n}(X_{i} - \overline{X})^{k}$\\

Пример:
Пусть $X ~ R[a,b]$, гже а, b - произвольные параметры. С помощью метода моментов построить точечные оценки для a, b.\\
1)\\
\begin{equation}
X ~ R[a, b] \Rightarrow f(x) = 
\begin{cases}
\frac{1}{b - a}, x \in [a ,b]\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Неизвестные параметры (a и b), следовательно требуется два уравнения:\\
\begin{equation}
\begin{cases}
m_{1}(a, b) = \hat{m_{1}}(\overrightarrow{X}) \text{ - относительно начального момента 1-го порядка}\\
m_{2}(a, b) = \hat{m_{2}}(\overrightarrow{X}) \text{ - относительно центрального момента второго порядка}\\
\end{cases}
\end{equation}

2) Найдём теоретические моменты:\\
$m_{1}(a,b) = MX = \frac{a + b}{2}$\\
$\circ{m_{2}}(a,b) = DX = \frac{(b - a)^{2}}{12}$\\

Запишем выборочные моменты:\\
$\hat{m_{1}}(a,b)=\frac{1}{n}\sum_{i=1}{n}X_{i} = \overline{X}$\\
$\mathring{m_{2}}(a,b) = \frac{1}{n}\sum_{i=1}{n}(X-\overline{X})^{2} = \hat{\sigma}^{2}(\overrightarrow{X})$\\
или\\
$\mathring{m_{2}}(a,b) = \frac{1}{n - 1}\sum_{i=1}{n}(X-\overline{X})^{2} = S^{2}(\overrightarrow{X})$\\

Используем $S^{2}$\\
3) Приходим к следующей системе уравнений:\\
\begin{equation}
\begin{cases}
\overline{X} = \frac{a + b}{2}\\
S^{2} = \frac{(b - a)^{2}}{12}\\
\end{cases}
\end{equation}
a, b = ?\\
Далее система решается стандартно\\

Замечание:\\
1) Поскольку выбранные моменты $\hat{m_{k}}(\overrightarrow{X})$ и $\hat{\mathring{m_{k}}}(\overrightarrow{X})$ являются состоятельными оценками соотвествтующих теоретических моментов, то можно показать, что в случае непрерывной зависимости решения системы из пункта (2) от $\hat{m_{k}}$ (или от $\mathring{m_{k}}$) оценки параметров, полученные с использованием этого метода также являются состоятельными.\\
2) Так как выборочные моменты степени k при $k \geqslant 2$ являются смещёнными оценками своих теоретических аналогов, то и оценки параметров, полученные с помощью метода моментов также могут быть смещёнными.

\section{Метод максимального правдоподобия}
Пусть:\\
1) X - случайная величина, зависящая от вектора параметров $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{r})$\\

Ранее было введено понятие функции правдоподобия случайной выборки X:\\
$L(\overrightarrow{X}, \overrightarrow{\theta}) = p(X_{1}, \overrightarrow{\theta}) \cdot ... \cdot p(X_{n}, \overrightarrow{\theta})$\\,
где:\\
\begin{equation}
p(X_{i}, \overrightarrow{\theta}) = 
\begin{cases}
f(X_{i}, \overrightarrow{\theta}), \text{если X - непрерывная случайная величина}\\
P(X = X_{i}), \text{если X - дискретная случайная величина}\\
\end{cases}
\end{equation}

Можно показать, что чем ближе значение вектора $\overrightarrow{\tilde{\theta}}$ к теоретическому значению вектора тета, тем большие значения принимает функция правдоподобия $L(\overrightarrow{x}, \overrightarrow{\theta})$\\

В методе максимального правдоподобия в качестве точечных оценок неизвестного параметра выступают значения, доставляющие максимальное значение функции правдоподобия. Для реализации метода необходимо решить задачу\\
$L(\overrightarrow{X},\overrightarrow{\theta}) \rightarrow max_{\overrightarrow{\theta}}$ ( * )\\
тогда:\\
$\hat{\overrightarrow{\theta}}(\overrightarrow{X}) = arg max_{\overrightarrow{\theta}} L(\overrightarrow{X}, \overrightarrow{\theta})$\\


Замечание:\\
1) Для решения задачи ( * ) можно использовать неоходимые условия дифференциальной функции нескольких переменных:\\
\begin{equation}
\begin{cases}
\frac{\delta L(\overrightarrow{X}, \overrightarrow{\theta})}{\delta \theta_{1}} = 0\\
\frac{\delta L(\overrightarrow{X}, \overrightarrow{\theta})}{\delta \theta_{2}} = 0\\
\end{cases}
\end{equation}
Данные уравнения называются уравнениями правдоподобия\\
2) Функция L является произведением n сомножетелей и работать с ней не всегда удобно. Поэтому часто вместо задачи ( * ) решют эквивалентную задачу ( ** ):\\
$ln L(\overrightarrow{X},\overrightarrow{\theta}) \rightarrow max_{\overrightarrow{\theta}}$\\
т.е.:\\
$\hat{\overrightarrow{\theta}}(\overrightarrow{X}) = arg max_{\overrightarrow{\theta}} ln L(\overrightarrow{X}, \overrightarrow{\theta})$\\
Данная замена эквивалентна, так как ln - монотонная возрастающая функция.\\	

Пример:\\
$X ~ R[a,b]$, a,b - неизвестные параметры, необходимо с помощью метода максимального правдоподобия построить точеные оценки параметров a и b.\\

Решение:\\
1)\\
\begin{equation}
X \tilde R[a, b] \Rightarrow f(x) = 
\begin{cases}
\frac{1}{b - a}, x \in [a ,b]\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Выпишем функцию правдоподобия $L(\overrightarrow{X}, a, b) = \text{так как X - непрерывная случайная величина} = f(X_{1}, a, b) \cdot ... \cdot f(X_{n}, a, b) = \frac{1}{b - a} \cdot \frac{1}{b - a} \cdot ... \cdot \frac{1}{b - a}  = \frac{1}{(b - a)^{n}}$\\
2) $ ln L = ln \frac{1}{(b - a)^{n}} = -n ln (b - a) \rightarrow max_{a,b}$\\
Уравнения правдоподобия:\\
\begin{equation}
\begin{cases}
\frac{\delta ln L}{\delta a} = \frac{n}{b - a} = 0\\
\frac{\delta ln L}{\delta b} = -\frac{n}{b - a} = 0\\
\end{cases}
\end{equation}

n = 0 - противоречие, возникло из-за того, что неверно записана формула для L.\\
Правильная запись формулы:\\
\begin{equation}
L(X, a, b) = 
\begin{cases}
\frac{1}{(b - a)^{n}}, if X(1) \geqslant a, X(n) \leqslant b\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Вычисление $\frac{\delta L(\overrightarrow{X}, a, b)}{\delta a}, \frac{\delta L(\overrightarrow{X}, a, b)}{\delta b}$ затруднительно, так как a и b также зависят от области, в которой L > 0\\
Попробуем в ''в лоб'' найти $max_{a,b} L(\overrightarrow{X},a,b)$\\
a) если\\
\begin{equation}
\begin{cases}
a \leqslant X_{(1)}\\
b \geqslant X_{(n)}\\
\end{cases}
\end{equation}
то L > 0, в противном случае L = 0. Так как $L \rightarrow max$, то условия на a, b в любом случае должны быть верны\\
b) при вычислении условий на a, b\\
$L = \frac{1}{(b - a)^{n}}$\\
Так как $L \rightarrow max$, то $b - a \rightarrow min$\\
Тогда приближаем к подзадаче:
\begin{equation}
\begin{cases}
b - a \rightarrow min\\
a \leqslant X_{(1)}\\
b \geqslant X_{(n)}\\
\end{cases}
\end{equation}

Таким образом:
\begin{equation}
\begin{cases}
\hat{a}(\overrightarrow{X}) = X_{(1)}\\
\hat{b}(\overrightarrow{X}) = X_{(n)}\\
\end{cases}
\end{equation}

\section{Интервальные оценки}
Основные понятия:\\
1) Рассматривается вторая задача математической статистики.\\

Ранее для решения этой задачи использовались точечные оценки. Тогда принимались равенства $\theta_{j} = \hat{\theta_{j}}(\overrightarrow{x}), j = \overline{1, k}$\\
Для некоторых статистик $\hat{\theta_{1}}, ..., \hat{\theta_{r}}$. Недостатком данного подходя является то, что он выдаёт информацию о вероятностных характеритиках точности оценивания неизвестных параметров. Кроме точечных оценок для решения второй задачи математической статистики используется другой подход. Для простоты будем считать, что у нас только один неизвестный параметр $r = 1, \overrightarrow{\theta} = (\theta_{1}) = (\theta) = \theta$.

Опеределение:\\
Интервальной оценкой параметра $\theta$ уровня $\gamma$ ($\gamma$-интервальной оценкой), $\gamma \in (0;1)$ называют пару интервальных статистик $\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X})$ таких, что:\\
$P\{\theta \in (\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X})\} = \gamma$\\

\chapter{Лекция 9}
Замечание:\\
1) Интервальная оценка является интервалом со случайными границами $\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X})$, который  накрывает инизвестно теоретическое значение параметра с вероятностью $\gamma$.
2) Вероятность совершить ошибку при построении интервальной оценки уровня $\gamma$:\\
$1 - \gamma = P\{\theta \notin (\uline{\theta}(\ora{X}), \oline{\theta}(\ora{X}))\}$\\
3) Вероятностной оценкой интервальной оценкой уровня гамма является случайная величина $l(\ora{X}) = \oline{\theta}(\ora{X}) - \uline{\theta}(\ora{X})$\\
4) Иногда удобно строить односторонние интервальные оценки.

Определение: нижней односторонней интервальной оценкой гамма-доверительной границей для параметра $\theta$ называют статистику $\uline{\theta}$ такую, что $P\{\theta > \uline{\theta}\} = \gamma$, для верхней оценкой аналогично.\\


Определение: гамма-доверительным интервалом (доверительным интервалом уровня гамма) для параметра тэта называют реализацию (выборочное значение) интервальной оценки уровня гамма для этого параметра, т.е. интервал $(\uline{\theta}(\ora{x}), \oline{\theta}(\ora{x}))$ с детерминированными границами.\\

Замечание: иногда там, гем это не будет приводить к путанцие, мы будем допускать вольность речи, не разделяя строго понятия интервала и интервальной оценки доверительного интервала.

\section{Построение интервальных оценок}

Пусть:\\
1) X - случайная величина, закон распределения которой известен с точностью до значения параметра $\theta \in \mathcal{R}$\\
Требуется построить интервальную оценку уровня гамма для параметра $\theta$.\\

Определение: по определению статистика $g(\ora{X}, \theta)$ называется центральной, если закон её распределения не зависит от неизвестного параметра $\theta$.\\

Пример:\\
$X ~ N(\theta, \sigma^2)$, где $\theta$ - неизвестно, $\sigma^2$ - известно.\\

Рассмотрим  статистику $g(\ora{X}, \theta) = \frac{\theta - \oline{X}}{\sigma}\sqrt{n}$.\\

Покажем, что g - центральная статистика.\\
a) $X_i ~ N(\theta, \sigma^2), i = \uline{1, n} \Rightarrow \oline{X} ~ N(?, ?)$\\
Следовательно: $g(\ora{X}, \theta) = \frac{\theta \sqrt{n}}{\sigma} - \frac{\sqrt{n}}{\sigma} \oline{X} ~ N(m_g, \sigma^2_g)$\\
б) найдём:\\
$m_y = M[g(\ora{X}, \theta)] = M[\frac{\theta \sqrt{n}}{\sigma} - \frac{\sqrt{n} \oline{X}}{\sigma}] = 0$\\
$\sigma^2_g = D[g(\ora{X}, \theta)] = D[\frac{\theta \sqrt{n}}{\sigma} - \frac{\sqrt{n} \oline{X}}{\sigma}] = \frac{n}{\sigma^2}D\oline{X} = 1$\\

т.е. $g(\ora{X}, \theta) ~ N(0, 1)$ не зависит от $\theta$\\
Следовательно g - центральная статистика.

\section{Общий алгоритм построения интервальной оценки с использованием центральной статистики}
Пусть:\\
1) $g(\ora{X}, \theta)$ - центральная статистика\\
2) $g(\ora{X}, \theta)$ - как функция параметра тета также является монотонно возрастающей\\
3) $F_g(y)$ - является монотонно возрастающей\\
4) Выбраны некоторые значения $\alpha_1, \alpha_2 > 0$ такие что $\alpha_1 + \alpha_2 = 1 - \gamma$, где гамма - заданный уровень доверия.\\

Замечание:\\
из условния (3) сделаем вывод, что уравнени $F_g(t) = \alpha$ для любого $\alpha \in (0, 1)$ имеет единственное решение $t = q_\alpha$ - квантиль уровня $\alpha$ случайной величины g $P\{X < q_\alpha\}=\alpha$.

\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{9_1}}
\end{figure}

\begin{figure}[H]
	\center{\includegraphics[scale=1.3]{9_2}}
\end{figure}

Из свойств непрерывных случайных величин следует: $P\{q_{\alpha_1} < g(\ora{X}, \theta) < q_{1 - \alpha_2}\} = \gamma$\\

Запишем событие, стоящее под вероятностью в эквивалентном виде:\\
$\{q_{\alpha_1} < g(\ora{X}, \theta) < q_{1 - \alpha_2}\} \Leftrightarrow \{g^{-1}(\ora{X}, q_{\alpha_1}) < \theta < g^{-1}(\ora{X}, q_{1 - \alpha_2})\}$\\
Выражаем из двойного неравенства $\theta$ , с учётом условия (2) знаки неравенств не изменияются.\\

Обозначим:\\
$\uline{\theta}(\ora{X}) = g^{-1}(\ora{X}, g_{\alpha_{1}})$\\
$\oline{\theta}(\ora{X}) = g^{-1}(\ora{X}, g_{1 - \alpha_{2}})$\\
Так как события эквивалентны, то:\\
$P\{\uline{\theta}(\ora{X}) < \theta < \oline{\theta}(\ora{X})\} = \gamma$\\


Замечения:\\
1) Построенная оценка зависят от  выбранных $\alpha_1, \alpha_2$, обычно используют $\alpha_1 = \alpha_2 = \frac{1 - \gamma}{2}$. Однако это не всегда так.\\
2) Если выбрать:\\
$\alpha_1 = 1 - \gamma, \alpha_2 = 0$, то $q_{1 - \alpha_{2}} = +\infty \Rightarrow \uline{\theta}(\ora{X}) = g^{-1}(\ora{X}, q_{1 - \gamma}), \oline\theta (\ora{X}) = +\infty$\\
Т.е. $\uline\theta \ora{X}$ - нижнаяя гамма-доверительная граница для тэты.\\
3) Аналогично при $\alpha_1 = 0, \alpha_2 = 1 - \gamma$\\
$\uline\theta (\ora{X}) = -\infty$\\
$\oline\theta (\ora{X}) = g^{-1} (\ora{X}, q_\gamma)$\\

Пример:\\
Пусть $X ~ N(m, \sigma^2)$\\
где m - неизвестно, $\sigma$ - неизвестно.\\
Построить гамма-доверительную интервальную оценку для m\\
Решение:\\
1) Рассмотрим статистику $g(\ora{X}, m) = \frac{m - \oline{X}}{\sigma}\sqrt{n} ~ N(0, 1)$\\
$\alpha_1 = \alpha_2 = \frac{1 - \gamma}{2}$\\
Тогда:\\
$P\{-u_{\frac{1 + \gamma}{2}} < \frac{m - \oline{X}}{\sigma}\sqrt{n} < u_{\frac{1 + \gamma}{2}}\} = \gamma$\\
$P\{\oline{X} - \frac{\sigma u_{\frac{1 + \gamma}{2}}}{\sqrt{n}} < m < \oline{X} + \frac{\sigma u_{\frac{1 + \gamma}{2}}}{\sqrt{n}}\} = \gamma$\\

Замечание:\\
1) Мы использовали $\alpha_1 = \alpha_2 = \frac{1 - \gamma}{2}$. Однако решая эту систему для произвольных альфа больших нуля мы получили бы результаты:\\
$\uline{m}\ora{X} = \oline{X} - \frac{\sigma u_{\alpha_1}}{\sqrt{n}}$\\
$\oline{m}\ora{X} = \oline{X} + \frac{\sigma u_{1 - \alpha_2}}{\sqrt{n}}$\\

В этом случае различные доверительные интервалы:\\
$l(\ora{X}) = \oline{m}(\ora{X}) - \uline{m}(\ora{X}) = \frac{\sigma}{\sqrt{n}}(u_{1 - \alpha_2} - u_{\alpha_1})$\\

Очевидно, что $l(\ora{X}) \rightarrow min, если \alpha_1 = \alpha_2$, в любом случае в рассматривемом примере размах является не случаной, а дискретной величиной.\\
2)  Зависимость размаха доверительного интервала от параметров:\\
Чем больше n, тем меньше размах.\\
При $\gamma \rightarrow 1, l \rightarrow +\infty$\\ 

Пример:\\
Пусть $X \tilde N(m, \sigma^2)$\\
где m - неизвестно, $\sigma$ - неизвестно.\\
Построить гамма-доверительную интервальную оценку для m\\
Решение:\\
1) Рассмотрим статистику $g(\ora{X}, m) = \frac{m - \oline{X}}{S(\ora{X})}\sqrt{n} ~ ?$\\
2) Представим в виде:\\
$g(\ora{X}, m) = \frac{\frac{m - \ora{X}}{\sigma}\sqrt{n}}{frac{S(\ora{X})}{\sigma}} = \frac{\frac{m - \oline{X}}{\sigma}\sqrt{n}}{\sqrt{\frac{(n - 1) S^2 (\ora{X})}{\sigma^2}}}\sqrt{n - 1}$\\
Пусть $\xi = \frac{m - \oline{X}}{\sigma}\sqrt{n}, \nu = \frac{(n - 1) S^2 (\ora{X})}{\sigma^2}$\\
Тогда $g(\ora{X}, m) = \frac{\xi}{\nu}\sqrt{n - 1}$\\
$\xi ~ N(0, 1), \nu ~ X^2 (n - 1)$, $\xi, \nu$ - независимы.
3) Таким образом g зависит от распределения Стьюдента от (n - 1)\\
4) Выражаем интервал через m и S аналогично предыдущему примеру и таким образом получаем интервальные оценки.\\
 
\chapter{Лекция 10}
Пример:\\
Пусть $X \tilde N(m, \sigma^2)$, где $\sigma^2$ - неизвестная, m - неизвестная. Построить интервальные оценки.

Рассмотрим статистику:\\
$g(\ora{X}, \sigma^2) = \frac{(n-1)\sigma^2 (\ora{X})}{\sigma^2}$\\

Можно показать, что $g \tilde X^2 (n - 1)$, т.о. принимаем $\alpha_1 = \alpha_2 = \frac{1 - \gamma}{2}$, получаем:\\
$P\{h_{\frac{1 - \gamma}{2}} < g(\ora{X}, \sigma^2 ) < L_{\frac{1 + \gamma}{2}}\} = \gamma$\\
$P\{h_{\frac{1 - \gamma}{2}} < \frac{(n-1)\sigma^2 (\ora{X})}{\sigma^2} < L_{\frac{1 + \gamma}{2}}\} = \gamma$\\
$P\{\frac{1}{h_{\frac{1 - \gamma}{2}}} > \frac{\sigma^2}{(n-1)\sigma^2 (\ora{X})} > \frac{1}{h_{\frac{1 + \gamma}{2}}}\} = \gamma$\\
$P\{\frac{(n-1)\sigma^2 (\ora{X})}{h_{\frac{1 + \gamma}{2}}} < \sigma^2 < \frac{(n-1)\sigma^2 (\ora{X})}{h_{\frac{1 - \gamma}{2}}}\} = \gamma$\\
т.е.:\\
$\uline\sigma^2 (\ora{X}) = \frac{(n-1)\sigma^2 (\ora{X})}{h_{\frac{1 + \gamma}{2}}}$\\
$\oline\sigma^2 (\ora{X}) = \frac{(n-1)\sigma^2 (\ora{X})}{h_{\frac{1 - \gamma}{2}}}$\\

\section{Модуль 2. Проверка параметрических гипотез. Основные понятия}
Пусть X - случайная величина, закон распределения которой неизвестен, или известен не полностью. По определению статистической гипотезой называется любое утверждение о законе распределения случайной величины X.\\

Определение - статистическая гипотеза называется простой, если она однозначно определяет закон распределения случайной величины X, т.е. однозначно определяет функцию распределения случайной величины X.\\

Определение - статистическая гипотеза называется параметрической, если она является утверждением относительно значений неизвестного параметра, общий вид которого известен. 

В противном случае гипотеза называется непараметрической.\\

Примеры:\\
1) Пусть известно, что случайная величина X имеет нормальное распределение. где параметры m и $\sigma^2$ - неизвестны. 

Рассмотрим гипотезу:\\
$H_1  = \{m = 0, \sigsq = 1\}$ - простая параметрическая гипотеза\\
$H_2 = \{m > 0, \sigsq = 1\}$ - сложная параметрическая\\
$H_3 = \{m = 1\}$ - сложная параметрическая\\

2) Пусть X - случайная величина, закон распределения которой неизвестен. Рассмотрим гипотезы:\\
$H_1  = \{X \tilde N(0,1)\}$ - простая непараметрическая гипотеза\\
$H_2 = \{X \tilde N(m, \sigsq)\}$ - сложная непараметрическая\\
$H_3 = \{N(1, \sigsq)\}$ - сложная непараметрическая\\

Задачу проверки гипотез обычно ставят следующим образом:\\
1) Выдвигают гипотезу $H_0$, которую называют основной\\
2) Затем выдвигают $H_1$, которую называют альтернативной или конкурирующей, при этом гипотезы $H_0$ и $H_1$ не пересекаются, но возможно их сумма не исчерпывает все возможные случаи\\
3) На основании выборки $\ora{x} \in X$ принимают решение об истинности либо основной гипотезы, либо конкурирующей.\\

Определение - правило, в соответствии с которым принимается решение об истинности гипотез $H_0, H_1$, называется критерием проверки гипотез. Как правило критерий задают с использованием критического множества. $W \subseteq X_n$, при этом соответствующее правило имеет вид:\\
если $\ora{x} \in W \Rarrow{}$ отклонить $H_0$, принять $H_1$\\
если $\ora{x} \in X_n \setminus W \Rarrow{}$ принять $H_0$, отклонить $H_1$\\

Замечание:\\
1) Критерий полностью задаётся критическим множеством $W$\\
2) Множество $X_n \setminus W$ называется доверительным\\

При использовании любого критеря возмодны ошибки двух видов:\\
1) Принять конкурирующую гипотезу при условии, что истинная основная:\\
$\alpha = P\{\ora{X} \in W | H_0\}$\\
2) Ошибка второго рода - принять основную гипотезу при истинности конкурирующей. Вероятность совершения этой ошибки:\\
$\beta = P\{\ora{X} \in X_n \setminus W | H_1\}$\\
Величину $1 - \beta$ называют мощностью критерия.\\

Замечание:\\
Конечно, хотелось бы построить критерий так, чтобы вероятности совершения обеих этих ошибок были минимальны, однако это невозможно, поэтому при построении критерия обычно фиксируют уровень значимости (т.е. $\alpha$) и максимизируют мощность. То есть минимизируют $\beta$ при фиксированном $\alpha$.

\section{Критерий Неймана-Пирсона приверки двух простых гипотез}
Пусть X - случайная величина, общий вид закона распределения которой известен с точностью до неизвестного параметра $\theta$.

Рассмотрим задачу проверки двух простых гипотез:\\
$H_0 = \{\theta = \theta_0\}$ против $H_1 = \{\theta = \theta_1\}$, где $\theta_0 \neq \theta_1$.\\
Введём статистику:\\
$\phi(\ora{X}) = \frac{L(\ora{X}, \theta_1)}{L(\ora{X}, \theta_0)}$\\
где L - функция правдоподобия случайной величины X.\\

Определение - статистику $\phi$ называют отношением правдоподобия.\\

Понятно, что чем больше занчение $\phi(\ora{x})$, тем более предпочтительной будет гипотеза $H_1$, поэтому критическое множество в рассматриваемой задаче будет иметь вид:\\
$W = \{\ora{x} \in X_n : \phi(\ora{x}) \geqslant C_\phi\}$\\
для некоторой подхощядей константы $C_\phi$, которая получается из уравнения:\\
$P\{\phi(\ora{X}) \geqslant C_\phi | H_0\} = \alpha$\\
Так как проверяется две простые гипотезы, то при выполнении условия $C_\phi$ значение $\beta$ вероятности совершит ошибку второго рода и будет отвергнуто однозначно.\\

Замечание - построенный критерий называется критерием Неймана-Пирсона для простых гипотез.\\

Замечание:\\
Если X - непрерывная случайная величина, то:\\
$P = \{\ora{X} \in W | H_0\} = \int ... \int\limits_{W} f_{\ora{X}} (t_1, ..., t_n, \theta_0)dt_1, ...., dt_n$, $\theta = 	\theta_0$, так как $H = H_0$\\
Так как $f_{\ora{X}}(t_1, ..., t_n, \theta) = f_{X_1}(t_1, ..., t_n, \theta) \cdot ... \cdot f_{X_n}(t_1, ..., t_n, \theta) = f(t_1, \theta) \cdot ... \cdot f(t_n, \theta) = $ по определению функции правдоподобия $ = L(t_1, ..., t_n, \theta)$\\
То:\\
$P = \{\ora{X} \in W | H_0\} = \int ... \int\limits_{W} f_{\ora{X}} (t_1, ..., t_n, \theta_0)dt_1, ...., dt_n =  \int ... \int\limits_{W}L (t_1, ..., t_n)dt_1, ...., dt_n = \alpha, \{(t_1, t_n): \phi(t_1, ..., t_n) \geqslant C_\phi\}$\\

Пример:\\
Пусть $X \tilde N(m, \sigsq),$ где сигма - известна, m - неизвестна. Рассмотрим задачу проверки гипотезы:\\
$H_0 = \{m = m_0\}$ против $H_1 = \{m = m_1\}$, где $m_1 > m_0$ (2 простые параметрические гипотезы), используем критерий Неймана-Пирсона. Составим отношение правдоподобия:\\
$\phi(\ora{X}) = \frac{L(\ora{X}, m_1)}{L(\ora{X}, m_0})$\\
Запишем $L(\ora{X}, m) = \frac{1}{\sqrt{2\pi} \sigma^n} e^{-\frac{1}{2\sigsq} \sum\limits_{i=1}^{n}(X_i - n_i)^2}$\\

Тогда $\phi(\ora{X}) = \frac{L(\ora{X}, m_1)}{L(\ora{X}, m_0}) = e^{-\frac{1}{\sigsq}[\sum\limits_{i=1}^{n}(X_i - m_1)^2 - \sum\limits_{i=1}^{n}(X_i - m_0)^2]} = ... = e^{\frac{m_1 - m_0}{2\sigsq}\sum 2X_i} \cdot  e^{-\frac{m_1 - m_0}{2\sigsq}\sum (m_0 + m_1)}$


























